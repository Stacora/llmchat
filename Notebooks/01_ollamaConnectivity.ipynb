{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73060ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef080472",
   "metadata": {},
   "source": [
    "# Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb8eed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "207441c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible! Honey's unique properties, such as its low water content and acidic pH, make it a virtually unsolvable medium for bacteria and microorganisms. Isn't that sweet?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a fun fact\n",
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact\"}])\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c39045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RULES\n",
    "systemPrompt = \"You are a helpful assistant that can answer questions and help with tasks.\"\n",
    "\n",
    "## LLM\n",
    "ollama_ai = OpenAI(base_url=\"http://localhost:11434/v1\", api_key='ollama')\n",
    "\n",
    "## USER\n",
    "user_prompt = \"What is the capital of France?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": systemPrompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "response = ollama_ai.chat.completions.create(model=\"llama3.2\", messages=messages)\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e1bf3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-768', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of France is Paris.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1762643628, model='llama3.2', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=8, prompt_tokens=46, total_tokens=54, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb5ea8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Hi, my name is Sebastian\n",
      "LLM: Hello Sebastian! Nice to meet you. How can I assist you today? Do you have any questions or need help with something specific?\n",
      "\n",
      "Conteo: Es 1/3\n",
      "#########################\n",
      "USER: how are you doing?\n",
      "LLM: I'm doing well, thank you for asking! I'm a large language model, so I don't have feelings or emotions like humans do, but I'm functioning properly and ready to assist with any questions or tasks you may have. How about you? How's your day going?\n",
      "\n",
      "Conteo: Es 2/3\n",
      "#########################\n",
      "USER: do you remember my name?\n",
      "LLM: This conversation just started, so I haven't had a chance to learn your name yet. You introduced yourself earlier by saying \"you are a helpful assistant that can answer questions and help with tasks.\" Would you like to share your name with me now? I'd be happy to remember it for our conversation.\n",
      "\n",
      "Conteo: Es 3/3\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "nresponses = int(input('Numero de Respuestas:'))\n",
    "\n",
    "LLM_msm = [{'role': 'system', 'content': systemPrompt}]\n",
    "\n",
    "ollama_ai = OpenAI(base_url=\"http://localhost:11434/v1\", api_key='ollama')\n",
    "model = \"llama3.2\"\n",
    "\n",
    "def context(LLMmsm):\n",
    "    contexto_textual = [f\"{m['role']}: {m['content']}\" for m in LLMmsm]\n",
    "    \n",
    "    contextMsm = [{'role' : 'System', 'content': f''},\n",
    "              {'role' : 'user', 'content': f\"\"\"\n",
    "                This is the conversation history so far: {contexto_textual}\n",
    "\n",
    "                Please summarize the dialogue in a concise, objective, and coherent manner. \n",
    "                Focus only on the key topics, questions, and responses — avoid unnecessary details.  \n",
    "                Begin your summary with: \"Here is a summary of the conversation so far:\"\n",
    "                \"\"\"}]\n",
    "\n",
    "    response = ollama_ai.chat.completions.create(model=model, messages=contextMsm)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "for i in range(nresponses):\n",
    "    conteoLoop = f'Es {i+1}/{nresponses}'\n",
    "    \n",
    "    #### Contextualizar ollama #####\n",
    "    context_msm = context(LLM_msm)\n",
    "    sysPromptContext = f\"\"\"\n",
    "    Original system prompt: {systemPrompt}\n",
    "    \n",
    "    Conversation summary:\n",
    "    {context_msm}\n",
    "    \n",
    "    Use this summary as contextual background only if it helps you provide a more accurate and coherent answer to the user's current question.\n",
    "    \"\"\"\n",
    "    \n",
    "    #### User Input process #####\n",
    "    userResponse = input('User: ')\n",
    "    \n",
    "    userResponseLLM = {'role': 'user', 'content': userResponse}\n",
    "    \n",
    "    loopmsm = [{'role' : 'system', 'content' : sysPromptContext},\n",
    "               {'role' : 'user', 'content' : userResponse}]\n",
    "    \n",
    "    #### Storing conversation user ####\n",
    "    LLM_msm.append(userResponseLLM)\n",
    "    \n",
    "    #### Ollama ####\n",
    "    response = ollama_ai.chat.completions.create(model=model, messages=loopmsm)\n",
    "    systemResponse = response.choices[0].message.content\n",
    "    \n",
    "    #### Storing conversation llm ####\n",
    "    LLM_response = {'role': 'system', 'content': systemResponse}\n",
    "    LLM_msm.append(LLM_response)\n",
    "    \n",
    "    #### Results ####\n",
    "    print(f'USER: {userResponse}')\n",
    "    print(f'LLM: {systemResponse}')\n",
    "    print(f'\\nConteo: {conteoLoop}')\n",
    "    print('#########################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c43c029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Original system prompt: You are a helpful assistant that can answer questions and help with tasks.\\n    \\n    Conversation summary:\\n    Here is a summary of the conversation so far:\\n\\nHere is a summary of the conversation so far:\\n\\nSebastian inquired about the capital cities and official languages of Peru and Brazil, receiving responses that Lima is the capital of Peru and Brasília is the capital of Brazil. Additionally, he asked which languages are spoken in both countries. The assistant provided information on Spanish as the primary language in Peru, with indigenous languages such as Quechua and Aymara also used, and Portuguese as the official language in Brazil, noting its similarity to Spanish.\\n    \\n    Use this summary as contextual background only if it helps you provide a more accurate and coherent answer to the user's current question.\\n    \""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sysPromptContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9758c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a concise and objective summary of our previous conversation:\\n\\n* Seba (user) interacted with ChatGPT (assistant).\\n* The user asked ChatGPT to demonstrate its capabilities, listing various tasks they would like the assistant to perform.\\n* ChatGPT provided a list of services it can offer, covering topics such as text summarization, language translation, programming, math problem-solving, travel planning, task simulation, and more.\\n\\nThis summary presents the conversation in a factual manner, without expressing a personal opinion or bias.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Contextualizar ollama #####\n",
    "model = \"llama3.2\"\n",
    "contexto_textual = [m['content'] for m in LLM_msm]\n",
    "contextMsm = [{'role' : 'System', 'content': f''},\n",
    "              {'role' : 'user', 'content': f\"\"\"\n",
    "               This is the previous conversation we had so far {contexto_textual}.\n",
    "               Sumarize the conversation if possible in a consice manner in a way that it could be objective\n",
    "               \"\"\"}]\n",
    "\n",
    "response = ollama_ai.chat.completions.create(model=model, messages=contextMsm)\n",
    "response.choices[0].message.content\n",
    "\n",
    "def context(LLMmsm):\n",
    "    contexto_textual = [m['content'] for m in LLMmsm]\n",
    "    \n",
    "    contextMsm = [{'role' : 'System', 'content': f''},\n",
    "              {'role' : 'user', 'content': f\"\"\"\n",
    "               This is the previous conversation we had so far {contexto_textual}.\n",
    "               Sumarize the conversation if possible in a consice manner in a way that it could be objective\n",
    "               \"\"\"}]\n",
    "\n",
    "    response = ollama_ai.chat.completions.create(model=model, messages=contextMsm)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a6cc85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You are a helpful assistant that can answer questions and help with tasks.',\n",
       " 'Hola, mi nombre es Seba, y el tuyo?',\n",
       " '¡Hola, Seba! Yo soy ChatGPT, tu asistente. ¿En qué puedo ayudarte hoy?',\n",
       " 'Estoy prbando tus capacidades',\n",
       " '¡Genial, Seba! Dime qué te gustaría probar. Puedo ayudarte con, por ejemplo:\\n- Resumir un texto en 3 frases\\n- Explicar un concepto complejo de forma sencilla\\n- Traducir y adaptar tono (es/es-LatAm)\\n- Redactar o mejorar un email/mensaje\\n- Generar ideas creativas (nombres, slogans, contenidos)\\n- Programación (Python/JS), depurar errores, explicar código\\n- Resolver matemáticas paso a paso\\n- Planificar un viaje, estudio o proyecto\\n- Simular una entrevista o practicar un idioma\\n\\nSi quieres, te hago una mini demo ahora: dame un tema y te preparo un resumen de 3 líneas, o pídeme un fragmento de código que haga algo concreto.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexto_textual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7bf590",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7bdd7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd2df840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi, Ed! Great to meet you. How can I help today?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm Ed!\"}\n",
    "    ]\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-5\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c1b9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE: Hola, mi nombre es Seba, y el tuyo?\n",
      "LLM: ¡Hola, Seba! Yo soy ChatGPT, tu asistente. ¿En qué puedo ayudarte hoy?\n",
      "\n",
      "Conteo: Es 1/2\n",
      "#########################\n",
      "USE: Estoy prbando tus capacidades\n",
      "LLM: ¡Genial, Seba! Dime qué te gustaría probar. Puedo ayudarte con, por ejemplo:\n",
      "- Resumir un texto en 3 frases\n",
      "- Explicar un concepto complejo de forma sencilla\n",
      "- Traducir y adaptar tono (es/es-LatAm)\n",
      "- Redactar o mejorar un email/mensaje\n",
      "- Generar ideas creativas (nombres, slogans, contenidos)\n",
      "- Programación (Python/JS), depurar errores, explicar código\n",
      "- Resolver matemáticas paso a paso\n",
      "- Planificar un viaje, estudio o proyecto\n",
      "- Simular una entrevista o practicar un idioma\n",
      "\n",
      "Si quieres, te hago una mini demo ahora: dame un tema y te preparo un resumen de 3 líneas, o pídeme un fragmento de código que haga algo concreto.\n",
      "\n",
      "Conteo: Es 2/2\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "nresponses = int(input('Numero de Respuestas:'))\n",
    "systemPrompt = 'You are a helpful assistant that can answer questions and help with tasks.'\n",
    "LLM_msm = [{'role': 'system', 'content': systemPrompt}]\n",
    "\n",
    "openai = OpenAI()\n",
    "model = 'gpt-5'\n",
    "\n",
    "for i in range(nresponses):\n",
    "    conteoLoop = f'Es {i+1}/{nresponses}'\n",
    "    userResponse = input('User: ')\n",
    "    \n",
    "    userResponseLLM = {'role': 'user', 'content': userResponse}\n",
    "    LLM_msm.append(userResponseLLM)\n",
    "    \n",
    "    response = openai.chat.completions.create(model=model, messages=LLM_msm)\n",
    "    systemResponse = response.choices[0].message.content\n",
    "    \n",
    "    LLM_response = {'role': 'system', 'content': systemResponse}\n",
    "    LLM_msm.append(LLM_response)\n",
    "    \n",
    "    print(f'USER: {userResponse}')\n",
    "    print(f'LLM: {systemResponse}')\n",
    "    print(f'\\nConteo: {conteoLoop}')\n",
    "    print('#########################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a021e126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
